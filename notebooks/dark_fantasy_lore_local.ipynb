{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üó°Ô∏è Dark Fantasy Lore Generator - Local Model Edition\n",
                "\n",
                "Este notebook ejecuta el generador de lore usando un modelo descargado de HuggingFace, evitando problemas de quota de API.\n",
                "\n",
                "**Requisitos:**\n",
                "- GPU Runtime (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)\n",
                "- Token de HuggingFace (para modelos con licencia)\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üì¶ 1. Instalar Dependencias"
            ],
            "metadata": {
                "id": "section1"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "!pip install -q transformers accelerate bitsandbytes torch\n",
                "!pip install -q huggingface_hub sentencepiece pyyaml\n",
                "\n",
                "print(\"‚úì Dependencias instaladas\")"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîë 2. Login en HuggingFace (Opcional)\n",
                "\n",
                "Solo necesario para modelos con licencia (Llama, Mistral, etc.)\n",
                "\n",
                "Obten√© tu token en: https://huggingface.co/settings/tokens"
            ],
            "metadata": {
                "id": "section2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "# Descomenta la siguiente l√≠nea y pega tu token\n",
                "# login(token=\"hf_xxxxxxxxxxxxxxxxxxxx\")\n",
                "\n",
                "# O usa el login interactivo:\n",
                "login()"
            ],
            "metadata": {
                "id": "hf_login"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ü§ñ 3. Cargar Modelo\n",
                "\n",
                "Opciones de modelos (descomentar la que prefieras):\n",
                "\n",
                "| Modelo | VRAM | Calidad |\n",
                "|--------|------|---------|\n",
                "| Mistral-7B-Instruct | ~8GB | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
                "| Qwen2.5-7B-Instruct | ~8GB | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
                "| SmolLM2-1.7B-Instruct | ~4GB | ‚≠ê‚≠ê‚≠ê (Colab Free) |"
            ],
            "metadata": {
                "id": "section3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "import torch\n",
                "\n",
                "# === ELEGIR MODELO ===\n",
                "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
                "# model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
                "# model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"  # Para Colab Free\n",
                "\n",
                "print(f\"Cargando modelo: {model_id}\")\n",
                "print(\"Esto puede tomar 5-10 minutos...\")\n",
                "\n",
                "# Quantizaci√≥n 4-bit para ahorrar VRAM\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "# Verificar VRAM usada\n",
                "!nvidia-smi --query-gpu=memory.used --format=csv\n",
                "print(\"\\n‚úì Modelo cargado exitosamente!\")"
            ],
            "metadata": {
                "id": "load_model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîß 4. Crear LLMService Local"
            ],
            "metadata": {
                "id": "section4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import json\n",
                "import re\n",
                "from typing import Optional\n",
                "\n",
                "class LocalLLMService:\n",
                "    \"\"\"\n",
                "    Servicio LLM local compatible con la estructura del proyecto.\n",
                "    Reemplaza las llamadas a Gemini API.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, model, tokenizer):\n",
                "        self.model = model\n",
                "        self.tokenizer = tokenizer\n",
                "        self.total_tokens_generated = 0\n",
                "\n",
                "    def count_tokens(self, text: str) -> int:\n",
                "        \"\"\"Contar tokens en un texto.\"\"\"\n",
                "        return len(self.tokenizer.encode(text))\n",
                "\n",
                "    def generate_content(\n",
                "        self,\n",
                "        prompt: str,\n",
                "        generation_config: dict = None,\n",
                "        retries: int = 3,\n",
                "        caller: str = \"unknown\"\n",
                "    ) -> Optional[str]:\n",
                "        \"\"\"\n",
                "        Generar contenido usando el modelo local.\n",
                "        Interfaz compatible con LLMService original.\n",
                "        \"\"\"\n",
                "        print(f\"\\n[{caller}] Generando respuesta...\")\n",
                "        input_tokens = self.count_tokens(prompt)\n",
                "        print(f\"[{caller}] Input tokens: {input_tokens}\")\n",
                "\n",
                "        # Preparar prompt en formato chat\n",
                "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
                "\n",
                "        try:\n",
                "            # Aplicar template del modelo\n",
                "            if hasattr(self.tokenizer, 'apply_chat_template'):\n",
                "                input_text = self.tokenizer.apply_chat_template(\n",
                "                    messages,\n",
                "                    tokenize=False,\n",
                "                    add_generation_prompt=True\n",
                "                )\n",
                "            else:\n",
                "                # Fallback para modelos sin chat template\n",
                "                input_text = f\"[INST] {prompt} [/INST]\"\n",
                "\n",
                "            inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "            # Generar\n",
                "            with torch.no_grad():\n",
                "                outputs = self.model.generate(\n",
                "                    **inputs,\n",
                "                    max_new_tokens=4096,\n",
                "                    temperature=0.85,\n",
                "                    top_p=0.95,\n",
                "                    do_sample=True,\n",
                "                    pad_token_id=self.tokenizer.eos_token_id,\n",
                "                    repetition_penalty=1.1\n",
                "                )\n",
                "\n",
                "            # Decodificar respuesta\n",
                "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "            # Extraer solo la respuesta del assistant\n",
                "            if \"[/INST]\" in full_response:\n",
                "                response = full_response.split(\"[/INST]\")[-1].strip()\n",
                "            elif \"assistant\" in full_response.lower():\n",
                "                response = full_response.split(\"assistant\")[-1].strip()\n",
                "            else:\n",
                "                # Remover el prompt del inicio\n",
                "                response = full_response[len(input_text):].strip()\n",
                "\n",
                "            output_tokens = self.count_tokens(response)\n",
                "            self.total_tokens_generated += output_tokens\n",
                "\n",
                "            print(f\"[{caller}] Output tokens: {output_tokens}\")\n",
                "            print(f\"[{caller}] ‚úì Generaci√≥n completada\")\n",
                "\n",
                "            return response\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"[{caller}] ‚úó Error: {e}\")\n",
                "            return None\n",
                "\n",
                "# Instanciar servicio\n",
                "llm_service = LocalLLMService(model, tokenizer)\n",
                "print(\"‚úì LocalLLMService creado\")"
            ],
            "metadata": {
                "id": "local_llm_service"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üì• 5. Descargar C√≥digo del Proyecto"
            ],
            "metadata": {
                "id": "section5"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Opci√≥n A: Clonar desde GitHub (si lo ten√©s subido)\n",
                "# !git clone https://github.com/TU_USUARIO/dark-fantasy-book-generator.git\n",
                "\n",
                "# Opci√≥n B: Crear estructura m√≠nima localmente\n",
                "import os\n",
                "\n",
                "os.makedirs(\"backend/agents\", exist_ok=True)\n",
                "os.makedirs(\"backend/services\", exist_ok=True)\n",
                "os.makedirs(\"backend/data\", exist_ok=True)\n",
                "\n",
                "print(\"‚úì Estructura de directorios creada\")\n",
                "print(\"\\nüìÅ Ahora sube los archivos del backend usando el panel de archivos de Colab\")\n",
                "print(\"   o usa la siguiente celda para subir un .zip\")"
            ],
            "metadata": {
                "id": "download_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Opci√≥n C: Subir archivo .zip con el backend\n",
                "from google.colab import files\n",
                "import zipfile\n",
                "\n",
                "print(\"Selecciona el archivo .zip con la carpeta 'backend'\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "for filename in uploaded.keys():\n",
                "    if filename.endswith('.zip'):\n",
                "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
                "            zip_ref.extractall('.')\n",
                "        print(f\"‚úì {filename} extra√≠do\")"
            ],
            "metadata": {
                "id": "upload_zip"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîå 6. Configurar Agentes con LLM Local"
            ],
            "metadata": {
                "id": "section6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import sys\n",
                "sys.path.insert(0, '/content')\n",
                "\n",
                "# Importar componentes\n",
                "from backend.agents.lore_state_manager import LoreStateManager\n",
                "from backend.agents.era_architect import EraArchitectAgent\n",
                "from backend.agents.faction_forge import FactionForgeAgent\n",
                "from backend.agents.soul_weaver import SoulWeaverAgent\n",
                "from backend.agents.conflict_designer import ConflictDesignerAgent\n",
                "from backend.agents.pathweaver import PathweaverAgent\n",
                "from backend.services.variety_injector import VarietyInjector\n",
                "\n",
                "print(\"‚úì M√≥dulos importados\")\n",
                "\n",
                "# Crear instancias con nuestro LLM local\n",
                "state_manager = LoreStateManager()\n",
                "variety_injector = VarietyInjector()\n",
                "\n",
                "era_agent = EraArchitectAgent(llm_service, state_manager)\n",
                "faction_agent = FactionForgeAgent(llm_service, state_manager)\n",
                "soul_agent = SoulWeaverAgent(llm_service, state_manager)\n",
                "conflict_agent = ConflictDesignerAgent(llm_service, state_manager)\n",
                "path_agent = PathweaverAgent(llm_service, state_manager)\n",
                "\n",
                "print(\"‚úì Agentes configurados con LLM local\")"
            ],
            "metadata": {
                "id": "setup_agents"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üó°Ô∏è 7. Generar Lore!"
            ],
            "metadata": {
                "id": "section7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# === CONFIGURACI√ìN DEL PROYECTO ===\n",
                "PROJECT_NAME = \"Chronicles of the Shattered Crown\"\n",
                "NUM_ERAS = 3\n",
                "NUM_FACTIONS = 4\n",
                "NUM_CHARACTERS = 5\n",
                "NUM_CONFLICTS = 3\n",
                "NUM_CHAPTERS_PER_ROUTE = 3\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"üó°Ô∏è GENERANDO LORE: {PROJECT_NAME}\")\n",
                "print(f\"{'='*50}\\n\")\n",
                "\n",
                "# Obtener seeds de variedad\n",
                "variety_seeds = variety_injector.get_generation_seeds()\n",
                "print(f\"Culturas: {variety_seeds['name_cultures']}\")\n",
                "print(f\"Emoci√≥n: {variety_seeds['emotion_seed']}\")\n",
                "print(f\"Est√©tica: {variety_seeds['aesthetic_seed']}\")\n",
                "\n",
                "# Inicializar state\n",
                "state_manager.set_project_info(PROJECT_NAME, \"dark_fantasy\")\n",
                "state_manager.set_variety_seeds(variety_seeds)"
            ],
            "metadata": {
                "id": "config_project"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Fase 1: Generar Eras\n",
                "print(\"\\nüìú FASE 1: Generando Eras...\")\n",
                "eras_result = era_agent.process(PROJECT_NAME, variety_seeds, num_eras=NUM_ERAS)\n",
                "\n",
                "if eras_result.get('eras'):\n",
                "    print(f\"\\n‚úì {len(eras_result['eras'])} eras generadas:\")\n",
                "    for era in eras_result['eras']:\n",
                "        print(f\"  - {era.get('name', 'Unknown')}\")\n",
                "else:\n",
                "    print(\"‚úó Error generando eras\")"
            ],
            "metadata": {
                "id": "phase1_eras"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Fase 2: Generar Facciones\n",
                "print(\"\\n‚öîÔ∏è FASE 2: Generando Facciones...\")\n",
                "factions_result = faction_agent.process(\n",
                "    PROJECT_NAME,\n",
                "    variety_seeds,\n",
                "    eras_result.get('eras', []),\n",
                "    num_factions=NUM_FACTIONS\n",
                ")\n",
                "\n",
                "if factions_result.get('factions'):\n",
                "    print(f\"\\n‚úì {len(factions_result['factions'])} facciones generadas:\")\n",
                "    for faction in factions_result['factions']:\n",
                "        print(f\"  - {faction.get('name', 'Unknown')}\")\n",
                "else:\n",
                "    print(\"‚úó Error generando facciones\")"
            ],
            "metadata": {
                "id": "phase2_factions"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Fase 3: Generar Personajes\n",
                "print(\"\\nüë§ FASE 3: Generando Personajes...\")\n",
                "characters_result = soul_agent.process(\n",
                "    PROJECT_NAME,\n",
                "    variety_seeds,\n",
                "    factions_result.get('factions', []),\n",
                "    num_characters=NUM_CHARACTERS\n",
                ")\n",
                "\n",
                "characters = characters_result.get('characters', [])\n",
                "if characters:\n",
                "    print(f\"\\n‚úì {len(characters)} personajes generados:\")\n",
                "    for char in characters:\n",
                "        print(f\"  - {char.get('name', 'Unknown')} ({char.get('archetype', 'Unknown')})\")\n",
                "else:\n",
                "    print(\"‚úó Error generando personajes\")"
            ],
            "metadata": {
                "id": "phase3_characters"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Fase 4: Generar Conflictos\n",
                "print(\"\\nüî• FASE 4: Generando Conflictos...\")\n",
                "conflicts_result = conflict_agent.process(\n",
                "    PROJECT_NAME,\n",
                "    variety_seeds,\n",
                "    factions_result.get('factions', []),\n",
                "    characters,\n",
                "    num_conflicts=NUM_CONFLICTS\n",
                ")\n",
                "\n",
                "conflicts = conflicts_result.get('conflicts', [])\n",
                "if conflicts:\n",
                "    print(f\"\\n‚úì {len(conflicts)} conflictos generados:\")\n",
                "    for conflict in conflicts:\n",
                "        print(f\"  - {conflict.get('name', 'Unknown')}\")\n",
                "else:\n",
                "    print(\"‚úó Error generando conflictos\")"
            ],
            "metadata": {
                "id": "phase4_conflicts"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Fase 5: Generar Rutas\n",
                "print(\"\\nüìñ FASE 5: Generando Rutas Narrativas...\")\n",
                "routes_result = path_agent.process(\n",
                "    PROJECT_NAME,\n",
                "    variety_seeds,\n",
                "    factions_result.get('factions', []),\n",
                "    characters,\n",
                "    conflicts,\n",
                "    num_chapters_per_route=NUM_CHAPTERS_PER_ROUTE\n",
                ")\n",
                "\n",
                "routes = routes_result.get('routes', {})\n",
                "if routes:\n",
                "    print(f\"\\n‚úì Rutas generadas:\")\n",
                "    for route_name, route_data in routes.items():\n",
                "        chapters = route_data.get('chapters', [])\n",
                "        print(f\"  - {route_name}: {len(chapters)} cap√≠tulos\")\n",
                "else:\n",
                "    print(\"‚úó Error generando rutas\")"
            ],
            "metadata": {
                "id": "phase5_routes"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üíæ 8. Guardar Resultados"
            ],
            "metadata": {
                "id": "section8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Obtener estado final\n",
                "final_state = state_manager.get_state()\n",
                "\n",
                "# Guardar como JSON\n",
                "output_filename = PROJECT_NAME.replace(' ', '_') + '_lore.json'\n",
                "with open(output_filename, 'w', encoding='utf-8') as f:\n",
                "    json.dump(final_state, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"\\n‚úì Lore guardado en: {output_filename}\")\n",
                "\n",
                "# Mostrar estad√≠sticas\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(\"üìä ESTAD√çSTICAS FINALES\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Tokens generados: {llm_service.total_tokens_generated:,}\")\n",
                "print(f\"Eras: {len(final_state.get('eras', []))}\")\n",
                "print(f\"Facciones: {len(final_state.get('factions', []))}\")\n",
                "print(f\"Personajes: {len(final_state.get('characters', []))}\")\n",
                "print(f\"Conflictos: {len(final_state.get('conflicts', []))}\")"
            ],
            "metadata": {
                "id": "save_results"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Descargar archivo\n",
                "from google.colab import files\n",
                "files.download(output_filename)"
            ],
            "metadata": {
                "id": "download_results"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîç 9. Preview del Lore Generado"
            ],
            "metadata": {
                "id": "section9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Mostrar preview del lore\n",
                "from IPython.display import display, Markdown\n",
                "\n",
                "state = state_manager.get_state()\n",
                "\n",
                "md_output = f\"# {state.get('project_name', 'Lore')}\\n\\n\"\n",
                "\n",
                "# Cosmology\n",
                "if state.get('cosmology'):\n",
                "    cosmo = state['cosmology']\n",
                "    md_output += \"## üåå Cosmology\\n\\n\"\n",
                "    md_output += f\"**Creation Myth:** {cosmo.get('creation_myth', 'N/A')}\\n\\n\"\n",
                "    md_output += f\"**Divine Forces:** {cosmo.get('divine_forces', 'N/A')}\\n\\n\"\n",
                "\n",
                "# Eras\n",
                "if state.get('eras'):\n",
                "    md_output += \"## ‚è≥ Eras\\n\\n\"\n",
                "    for era in state['eras']:\n",
                "        md_output += f\"### {era.get('name', 'Unknown Era')}\\n\"\n",
                "        md_output += f\"{era.get('summary', '')}\\n\\n\"\n",
                "\n",
                "# Factions\n",
                "if state.get('factions'):\n",
                "    md_output += \"## ‚öîÔ∏è Factions\\n\\n\"\n",
                "    for faction in state['factions']:\n",
                "        md_output += f\"### {faction.get('name', 'Unknown')}\\n\"\n",
                "        md_output += f\"{faction.get('description', faction.get('summary', ''))}\\n\\n\"\n",
                "\n",
                "# Characters\n",
                "if state.get('characters'):\n",
                "    md_output += \"## üë§ Characters\\n\\n\"\n",
                "    for char in state['characters']:\n",
                "        md_output += f\"### {char.get('name', 'Unknown')} ({char.get('archetype', 'Unknown')})\\n\"\n",
                "        md_output += f\"{char.get('summary', char.get('background', ''))}\\n\\n\"\n",
                "\n",
                "display(Markdown(md_output[:5000]))  # Limitar preview"
            ],
            "metadata": {
                "id": "preview_lore"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}